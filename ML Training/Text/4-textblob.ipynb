{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 : TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob aims to provide access to common text-processing operations \n",
      "through a familiar interface. You can treat TextBlob objects as if they were Python \n",
      "strings that learned how to do Natural Language Processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "# setup nltk data\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "\n",
    "text = \"\"\"TextBlob aims to provide access to common text-processing operations \n",
    "through a familiar interface. You can treat TextBlob objects as if they were Python \n",
    "strings that learned how to do Natural Language Processing.\n",
    "\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TextBlob', 'aims', 'to', 'provide', 'access', 'to', 'common', 'text-processing', 'operations', 'through', 'a', 'familiar', 'interface', 'You', 'can', 'treat', 'TextBlob', 'objects', 'as', 'if', 'they', 'were', 'Python', 'strings', 'that', 'learned', 'how', 'to', 'do', 'Natural', 'Language', 'Processing']\n",
      "\n",
      "[Sentence(\"TextBlob aims to provide access to common text-processing operations \n",
      "through a familiar interface.\"), Sentence(\"You can treat TextBlob objects as if they were Python \n",
      "strings that learned how to do Natural Language Processing.\")]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tb.words)\n",
    "print()\n",
    "\n",
    "print(tb.sentences)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love bigmacs ==> Sentiment(polarity=0.5, subjectivity=0.6)\n",
      "I hate this traffic! ==> Sentiment(polarity=-1.0, subjectivity=0.9)\n",
      "American Idol is awesome! ==> Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "this song is lame ==> Sentiment(polarity=-0.5, subjectivity=0.75)\n",
      "Macy's is crap ==> Sentiment(polarity=-0.8, subjectivity=0.8)\n",
      "Macy's is the crap ==> Sentiment(polarity=-0.8, subjectivity=0.8)\n",
      "I don't don't love you ==> Sentiment(polarity=0.5, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"I love bigmacs\",  \n",
    "          \"I hate this traffic!\",  \n",
    "          \"American Idol is awesome!\", \n",
    "          \"this song is lame\", \n",
    "          \"Macy's is crap\",\n",
    "         \"Macy's is the crap\",\n",
    "         \"I love you a little\"]\n",
    "\n",
    "for tweet in tweets:\n",
    "    tb = TextBlob(tweet)\n",
    "    print(\"{} ==> {}\".format(tweet, tb.sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflection and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "dogs\n",
      "men\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "words = [\"cat\", \"dog\", \"man\", \"person\"]\n",
    "for w in words:\n",
    "    print(Word(w).pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# lematize\n",
    "from textblob import Word\n",
    "print(Word(\"went\").lemmatize('v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet integration / Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an open-source version of the UNIX operating system']\n",
      "\n",
      "- a game played on a court by two opposing teams of 5 players; points are scored by throwing the ball through an elevated horizontal hoop\n",
      "- an inflated ball used in playing basketball\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "print(Word(\"linux\").define())\n",
    "print()\n",
    "\n",
    "for d in Word(\"basketball\").definitions:\n",
    "    print(\"- \" + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'it': 2, 'was': 2, 'a': 3, 'sunny': 2, 'day': 2, 'we': 1, 'went': 1, 'to': 2, 'the': 2, 'dog': 2, 'park': 1, 'lots': 1, 'of': 1, 'dogs': 1, 'were': 1, 'running': 1, 'around': 1, 'my': 1, 'likes': 1, 'run': 1, 'too': 1, 'so': 1, 'he': 1, 'had': 1, 'great': 1, 'time': 1, 'i': 1, 'bought': 1, 'ice': 2, 'cream': 2, 'from': 1, 'truck': 1, 'yummy': 1, 'perfect': 1})\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "print(tb.word_counts)\n",
    "print()\n",
    "\n",
    "print(tb.word_counts['sunny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2 grams\n",
      "[WordList(['It', 'was']), WordList(['was', 'a']), WordList(['a', 'sunny']), WordList(['sunny', 'day']), WordList(['day', 'We']), WordList(['We', 'went']), WordList(['went', 'to']), WordList(['to', 'the']), WordList(['the', 'dog']), WordList(['dog', 'park']), WordList(['park', 'Lots']), WordList(['Lots', 'of']), WordList(['of', 'dogs']), WordList(['dogs', 'were']), WordList(['were', 'running']), WordList(['running', 'around']), WordList(['around', 'My']), WordList(['My', 'dog']), WordList(['dog', 'likes']), WordList(['likes', 'to']), WordList(['to', 'run']), WordList(['run', 'too']), WordList(['too', 'so']), WordList(['so', 'he']), WordList(['he', 'had']), WordList(['had', 'a']), WordList(['a', 'great']), WordList(['great', 'time']), WordList(['time', 'I']), WordList(['I', 'bought']), WordList(['bought', 'ice']), WordList(['ice', 'cream']), WordList(['cream', 'from']), WordList(['from', 'the']), WordList(['the', 'ice']), WordList(['ice', 'cream']), WordList(['cream', 'truck']), WordList(['truck', 'Yummy']), WordList(['Yummy', 'It']), WordList(['It', 'was']), WordList(['was', 'a']), WordList(['a', 'perfect']), WordList(['perfect', 'sunny']), WordList(['sunny', 'day'])]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "\n",
    "print(\"n=2 grams\")\n",
    "print(tb.ngrams(n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection & Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just had dinner\n",
      "to Spanish : Acabo de cenar \n",
      "to Japanese : 私はちょうど夕食 \n",
      "Language detection : en \n"
     ]
    }
   ],
   "source": [
    "text_en = \"I just had dinner\"\n",
    "print(text_en)\n",
    "print(\"to Spanish : {} \".format (TextBlob(text_en).translate(to='es')))\n",
    "print(\"to Japanese : {} \".format(TextBlob(text_en).translate(to='ja')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_jp = \"hello\"\n",
    "print(\"Language detection : {} \".format(TextBlob(text_jp).detect_language()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
